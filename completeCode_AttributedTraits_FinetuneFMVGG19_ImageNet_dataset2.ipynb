{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaZrBYdkGrv/BH/yPf+JTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samadkhani/ImageRecommendation/blob/main/completeCode_AttributedTraits_FinetuneFMVGG19_ImageNet_dataset2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcXwCVfEw1D5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "from keras.preprocessing.image import load_img\n",
        "import warnings\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "from tensorflow.keras.applications import VGG19\n",
        "# from keras import models\n",
        "# from keras import layers\n",
        "from scipy import io\n",
        "import scipy.io as spio\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "Img_size = 160\n",
        "read_images = []\n",
        "Lable_images = []\n",
        "list_OpenUser = []\n",
        "Lable_list_OpenUser = []\n",
        "\n",
        "# Program extracting first column\n",
        "import xlrd\n",
        "xlrd.xlsx.ensure_elementtree_imported(False, None)\n",
        "xlrd.xlsx.Element_has_iter = True\n",
        "\n",
        "OpennesUser = []\n",
        "\n",
        "loc = (\"/content/gdrive/MyDrive/PsychoFlickr_DataSet/AttributedTraits.xlsx\")\n",
        "\n",
        "wb = xlrd.open_workbook(loc)\n",
        "sheet = wb.sheet_by_index(0)\n",
        "sheet.cell_value(0, 0)\n",
        "\n",
        "for i in range(sheet.nrows):\n",
        "  OpennesUser.append(sheet.cell_value(i, 0)) # 0:Openness , 1: Conscientiousness, 2: Extraversion, 3: Neuroticism, 4: Agreeableness\n",
        "\n",
        "\n",
        "\n",
        "t_quartile = np.quantile(OpennesUser, 0.75)\n",
        "print(t_quartile)\n",
        "first_quartile = np.quantile(OpennesUser, 0.25)\n",
        "print(first_quartile)\n",
        "\n",
        "\n",
        "numOpennesUser = 0\n",
        "\n",
        "high_OpennesUser = 0\n",
        "low_OpennesUser = 0\n",
        "list_OpennesUser = []\n",
        "Lable_list_OpennesUser = []\n",
        "for i in range(len(OpennesUser)):\n",
        "  flag = 0\n",
        "  if OpennesUser[i] >= t_quartile:\n",
        "    flag = 1\n",
        "    high_OpennesUser = high_OpennesUser + 1\n",
        "    list_OpennesUser.append(i)\n",
        "    Lable_list_OpennesUser.append(1)\n",
        "  elif  OpennesUser[i]<=first_quartile:\n",
        "    flag = 1\n",
        "    low_OpennesUser = low_OpennesUser + 1\n",
        "    list_OpennesUser.append(i)\n",
        "    Lable_list_OpennesUser.append(-1)\n",
        "\n",
        "  if flag == 1:\n",
        "    numOpennesUser = numOpennesUser + 1\n",
        "\n",
        "print(numOpennesUser)\n",
        "print(low_OpennesUser)\n",
        "print(high_OpennesUser)\n",
        "\n",
        "\n",
        "print(len(list_OpennesUser))\n",
        "print(len(Lable_list_OpennesUser))\n",
        "\n",
        "\n",
        "base_path = '/content/gdrive/MyDrive/PsychoFlickr_DataSet/All_Users'\n",
        "\n",
        "lst = os.listdir(base_path)\n",
        "lst.sort()\n",
        "\n",
        "numImg = 0\n",
        "l = -1\n",
        "for i in lst:\n",
        "    vid_name = os.path.join(base_path, i)\n",
        "    flag = 0\n",
        "    l = l + 1\n",
        "    if OpennesUser[l] >= t_quartile:\n",
        "      flag = 1\n",
        "      list_OpenUser.append(i)\n",
        "      Lable_list_OpenUser.append(1)\n",
        "      Lab = 1\n",
        "    elif  OpennesUser[l]<=first_quartile:\n",
        "      flag = 1\n",
        "      list_OpenUser.append(i)\n",
        "      Lable_list_OpenUser.append(0)\n",
        "      Lab = 0\n",
        "\n",
        "    if flag == 1:\n",
        "      #print(vid_name)\n",
        "      print(\"numImg\")\n",
        "      print(vid_name)\n",
        "      print(numImg)\n",
        "      numImg = 0\n",
        "      for j in os.listdir(vid_name):  # read all as' inside A\n",
        "          img_path = os.path.join(vid_name, j)\n",
        "\n",
        "          numImg = numImg + 1\n",
        "\n",
        "          # load the image via load_img function\n",
        "          img = load_img(img_path)\n",
        "          resized_image = img.resize((Img_size, Img_size))\n",
        "          img_numpy_array = img_to_array(resized_image)\n",
        "          read_images.append(img_numpy_array)\n",
        "          Lable_images.append(Lab)\n",
        "\n",
        "    elif flag == 0:\n",
        "      continue\n",
        "\n",
        "xData = np.array(read_images)\n",
        "xData.shape\n",
        "\n",
        "y_Data = np.array(Lable_images)\n",
        "y_Data.shape\n",
        "\n",
        "num_classes = 2\n",
        "y_Data = keras.utils.to_categorical(y_Data, num_classes)\n",
        "y_Data.shape\n",
        "\n",
        "# split into train test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_testVal, y_train, y_testVal = train_test_split(xData, y_Data, test_size=0.2, random_state=40)\n",
        "print(X_train.shape, X_testVal.shape, y_train.shape, y_testVal.shape)\n",
        "\n",
        "X_test, X_Val, y_test, y_Val = train_test_split(X_testVal, y_testVal, test_size=0.25, random_state=40)\n",
        "print(X_test.shape, X_Val.shape, y_test.shape, y_Val.shape)\n",
        "\n",
        "\n",
        "conv_base = VGG19(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(Img_size, Img_size, 3))\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(4096, activation='relu', input_dim=5 * 5 * 512))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable weights '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "    if layer.name == 'block4_conv1':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "\n",
        "print('This is the number of trainable weights '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "batch_size = 30\n",
        "num_epochs = 20\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 5:\n",
        "        return 2e-7\n",
        "    elif epoch < 20:\n",
        "        return 2e-6\n",
        "\n",
        "\n",
        "learning_rate_scheduler = LearningRateScheduler(scheduler, verbose=1)\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = optimizers.RMSprop()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, label_smoothing=0.1\n",
        "        ),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=X_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data= (X_Val, y_Val),\n",
        "        callbacks=[checkpoint_callback, learning_rate_scheduler],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history , model\n",
        "\n",
        "\n",
        "cct_model = model\n",
        "history , model= run_experiment(cct_model)\n",
        "\n",
        "\n",
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# UserImages\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.activations import *\n",
        "import keras.backend as K\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def get_datatest():\n",
        "    X = []\n",
        "    y = []\n",
        "    k = 0\n",
        "\n",
        "    for j in os.listdir('/content/gdrive/My Drive/UserImages/'):\n",
        "      for i in os.listdir('/content/gdrive/My Drive/UserImages/'  + str(j) + '/'):\n",
        "        name = '/content/gdrive/My Drive/UserImages/'  + str(j) + '/'  + str(i)\n",
        "        print(name)\n",
        "        try :\n",
        "          img = Image.open(name)\n",
        "          k = k + 1\n",
        "          print(k)\n",
        "        except Exception:\n",
        "          continue\n",
        "        img = img.resize((Img_size, Img_size)).convert(\"RGB\")\n",
        "        X.append(np.array(img))\n",
        "\n",
        "    X = np.array(X)\n",
        "    return X , k\n",
        "\n",
        "UserImages , k = get_datatest()\n",
        "print(UserImages.shape)\n",
        "\n",
        "\n",
        "predict_UserImages = model.predict(UserImages)\n",
        "print(predict_UserImages.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%cd /content/gdrive/My Drive/Colab Notebooks\n",
        "io.savemat(\"predict_Personality_O_FMfinetunvgg19_Data2.mat\", {\"array\": predict_UserImages})\n"
      ]
    }
  ]
}