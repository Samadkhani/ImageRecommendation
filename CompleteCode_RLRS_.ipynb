{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samadkhani/ImageRecommendation/blob/main/CompleteCode_RLRS_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7LS7OVDa7PQ"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Importing necessary libraries\n",
        "import datetime\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "import scipy.io as spio\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import random\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.python.keras.layers import Concatenate , Dense, Input, Dropout, Add, Activation\n",
        "#from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.python.keras import Model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "#from buffer import BasicBuffer_a,BasicBuffer_b\n",
        "from sys import exit\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Changing directory to the location of the data\n",
        "%cd /content/gdrive/MyDrive/TestTrain_3Features\n",
        "\n",
        "# Loading the data\n",
        "matTrain = io.loadmat('Train_user__1.mat')\n",
        "matTest = io.loadmat('Test_user__1.mat')\n",
        "mTrainData = matTrain['Train_user__1']\n",
        "mTestData = matTest['Test_user__1']\n",
        "\n",
        "mTrain = np.zeros((1100, 108))\n",
        "mTest = np.zeros((1100, 108))\n",
        "print(mTrain.shape)\n",
        "print(mTest.shape)\n",
        "\n",
        "mTrain[:,0:107] = mTrainData[:,0:107]\n",
        "mTest[:,0:107] = mTestData[:,0:107]\n",
        "\n",
        "mTrain[:,-1] = mTrainData[:,-1]\n",
        "mTest[:,-1] = mTestData[:,-1]\n",
        "\n",
        "print(mTrain.shape)\n",
        "print(mTest.shape)\n",
        "\n",
        "\n",
        "UserID = 1\n",
        "num_Features = 107\n",
        "\n",
        "\n",
        "\n",
        "bz = 10 # batch_size\n",
        "\n",
        "start_step = 1000 # start_steps\n",
        "\n",
        "n_t_e = 400 # num_train_episodes\n",
        "\n",
        "mel = 100\n",
        "\n",
        "replay_sizeee = n_t_e * mel\n",
        "\n",
        "lr = 1e-3\n",
        "gam=0.5\n",
        "s_r = 0.5\n",
        "\n",
        "K_items =1\n",
        "\n",
        "hidden_sizes_1=(256,128)\n",
        "hidden_sizes_2=(100,64, 32)\n",
        "\n",
        "def CalculateReward(ImgData, UserID):\n",
        "  Reward = 0\n",
        "  #numUsers = 20    ########################################### numUsers #######################################\n",
        "  allReward = []\n",
        "  numData , m1 = ImgData.shape\n",
        "  for i in range(numData):\n",
        "    classData = ImgData[i , -1]\n",
        "    if classData == UserID:\n",
        "      Reward = 1\n",
        "    else:\n",
        "      Reward = -1\n",
        "    allReward.append(Reward)\n",
        "  #newImgData = np.hstack((ImgData,allReward))\n",
        "  return allReward\n",
        "\n",
        "RewardDataaaaTrain = CalculateReward(mTrain , UserID)\n",
        "RewardDataaaaTest = CalculateReward(mTest , UserID)\n",
        "RewardDataTrain = np.array(RewardDataaaaTrain)\n",
        "RewardDataTest = np.array(RewardDataaaaTest)\n",
        "\n",
        "\n",
        "# Concat Reward and Data\n",
        "\n",
        "def concatReward_Data(Data, RewardData):\n",
        "  numData , m1 = Data.shape\n",
        "  DatawithReward = np.zeros((numData,m1+1))\n",
        "\n",
        "  for i in range(numData):\n",
        "    t = Data[i,:]  #     ERORR      t = mTrain[i,:]\n",
        "    Rt = RewardData[i]  #    ERROR      Rt = RewardDataTrain[i]\n",
        "    DatawithReward[i,:] = np.hstack((t,Rt))\n",
        "\n",
        "  return DatawithReward\n",
        "\n",
        "#--------------------------------\n",
        "Data_Reward_Train = concatReward_Data(mTrain, RewardDataTrain)\n",
        "Data_Reward_Test = concatReward_Data(mTest, RewardDataTest)\n",
        "\n",
        "# reset initial state\n",
        "def resetFun(Data , UserID, state_dim):\n",
        "  numData , m1 = Data.shape\n",
        "  init_state = []\n",
        "  indx = 0\n",
        "  for i in range(numData):\n",
        "    if (Data[i , -1] == UserID):\n",
        "      init_state = Data[i,:state_dim]\n",
        "      indx = i\n",
        "      break\n",
        "\n",
        "  return init_state, indx\n",
        "\n",
        "\n",
        "def representation_state(pos_state):\n",
        "  #print(\"pos_state\")\n",
        "  #print(pos_state)\n",
        "  size_pos = pos_state.size\n",
        "  if size_pos>num_Features :\n",
        "    mean_pos = np.mean(pos_state, axis = 0)\n",
        "  elif size_pos== num_Features:\n",
        "    mean_pos = pos_state\n",
        "\n",
        "  return mean_pos\n",
        "\n",
        "\n",
        "from numpy import dot\n",
        "from scipy.special import softmax\n",
        "\n",
        "def selfAttention_representation(aa, pos_state):\n",
        "  size_pos = pos_state.size\n",
        "  pos_state_array = np.array(pos_state)\n",
        "  n , m = pos_state_array.shape\n",
        "  #print(\"pos_state_array\", pos_state_array.shape)\n",
        "  #print(\"n=\", n)\n",
        "  #print(\"m=\", m)\n",
        "\n",
        "  scoresj = np.zeros((n,))\n",
        "  attention = np.zeros((m,))\n",
        "\n",
        "  for j in range (n):\n",
        "    scoresj[j] = np.dot(pos_state_array[j,:] , aa)\n",
        "\n",
        "  #print(\"scoresj\", scoresj.shape)\n",
        "  #print(\"aa\", aa.shape[0])\n",
        "  # computing the weights by a softmax operation\n",
        "\n",
        "  weights = softmax(scoresj)\n",
        "\n",
        "  # computing the attention by a weighted sum of the value vectors\n",
        "  for j in range (n):\n",
        "    pos_state_array[j,:] = (np.dot(weights[j] , pos_state_array[j,:]))\n",
        "\n",
        "  attention = np.sum(pos_state_array, axis=0)\n",
        "  return attention\n",
        "\n",
        "\n",
        "def stepFun( a , Inx_I_2 , s , pos_state, UserID, Data, action_space_withInx, dis_trueRcomme, pos_state_All_epoch, Pos_Neg_state_Data,\n",
        "            Pos_Neg_state_Lable, All_Pos_Neg_state_Data, All_Pos_Neg_state_Lable):\n",
        "\n",
        "  rKitems = 0\n",
        "  size_a = a.size\n",
        "\n",
        "  for i_a in range (size_a) :\n",
        "    aaa = Data[int(a[i_a]), 0:num_Features + 1]\n",
        "\n",
        "    action_space_withInx = np.delete(action_space_withInx, int(Inx_I_2[i_a]), axis=0)\n",
        "\n",
        "    recommData = Data[int(a[i_a]) , :]\n",
        "    rKitems = rKitems + recommData[-1]\n",
        "    r = recommData[-1]\n",
        "\n",
        "    if recommData[num_Features] == UserID:\n",
        "\n",
        "      pos_state = pos_state.reshape(-1)\n",
        "      pos_state = np.append(pos_state, recommData[0:num_Features], axis=0)\n",
        "      pos_state = np.reshape(pos_state, (-1 , num_Features))\n",
        "\n",
        "      pos_state_All_epoch = pos_state_All_epoch.reshape(-1)\n",
        "      pos_state_All_epoch = np.append(pos_state_All_epoch, recommData[0:num_Features], axis=0)\n",
        "      pos_state_All_epoch = np.reshape(pos_state_All_epoch, (-1 , num_Features))\n",
        "\n",
        "      Pos_Neg_state_Data = Pos_Neg_state_Data.reshape(-1)\n",
        "      Pos_Neg_state_Data = np.append(Pos_Neg_state_Data, recommData[0:num_Features], axis=0)\n",
        "      Pos_Neg_state_Data = np.reshape(Pos_Neg_state_Data, (-1 , num_Features))\n",
        "\n",
        "      Pos_Neg_state_Lable = np.append(Pos_Neg_state_Lable, r)\n",
        "\n",
        "      #--------------------------------------\n",
        "      All_Pos_Neg_state_Data = All_Pos_Neg_state_Data.reshape(-1)\n",
        "      All_Pos_Neg_state_Data = np.append(All_Pos_Neg_state_Data, recommData[0:num_Features], axis=0)\n",
        "      All_Pos_Neg_state_Data = np.reshape(All_Pos_Neg_state_Data, (-1 , num_Features))\n",
        "\n",
        "      All_Pos_Neg_state_Lable = np.append(All_Pos_Neg_state_Lable, r)\n",
        "\n",
        "      All_Pos_Neg_state_Data, indices = np.unique(All_Pos_Neg_state_Data, return_index=True, axis=0)\n",
        "      All_Pos_Neg_state_Lable = All_Pos_Neg_state_Lable[indices]\n",
        "      #--------------------------------------\n",
        "\n",
        "      s_ = (( recommData[0:num_Features]) + selfAttention_representation( recommData[0:num_Features], pos_state))/2\n",
        "\n",
        "    else:\n",
        "      s_ = representation_state(pos_state)  # + 0.001 * s\n",
        "\n",
        "      Pos_Neg_state_Data = Pos_Neg_state_Data.reshape(-1)\n",
        "      Pos_Neg_state_Data = np.append(Pos_Neg_state_Data, recommData[0:num_Features], axis=0)\n",
        "      Pos_Neg_state_Data = np.reshape(Pos_Neg_state_Data, (-1 , num_Features))\n",
        "\n",
        "      Pos_Neg_state_Lable = np.append(Pos_Neg_state_Lable, r)\n",
        "\n",
        "      #--------------------------------------\n",
        "      All_Pos_Neg_state_Data = All_Pos_Neg_state_Data.reshape(-1)\n",
        "      All_Pos_Neg_state_Data = np.append(All_Pos_Neg_state_Data, recommData[0:num_Features], axis=0)\n",
        "      All_Pos_Neg_state_Data = np.reshape(All_Pos_Neg_state_Data, (-1 , num_Features))\n",
        "\n",
        "      All_Pos_Neg_state_Lable = np.append(All_Pos_Neg_state_Lable, r)\n",
        "\n",
        "      #print(All_Pos_Neg_state_Data.shape)\n",
        "\n",
        "      All_Pos_Neg_state_Data, indices = np.unique(All_Pos_Neg_state_Data, return_index=True, axis=0)\n",
        "      All_Pos_Neg_state_Lable = All_Pos_Neg_state_Lable[indices]\n",
        "      #--------------------------------------\n",
        "\n",
        "  return (s_ , rKitems, pos_state, action_space_withInx, dis_trueRcomme, pos_state_All_epoch, Pos_Neg_state_Data, Pos_Neg_state_Lable,\n",
        "          All_Pos_Neg_state_Data, All_Pos_Neg_state_Lable)\n",
        "\n",
        "\n",
        "\n",
        "def changeLR(epoch):\n",
        "  if epoch < 50:\n",
        "    return 0.01\n",
        "  elif epoch< 200:\n",
        "    return 0.001\n",
        "  elif epoch< 400:\n",
        "    return 0.001\n",
        "  else:\n",
        "    return 0.0001\n",
        "\n",
        "\n",
        "\n",
        "# simple NN Generator\n",
        "def ANN2(input_shape,layer_sizes, hidden_activation='relu', output_activation=None, dropout=0.2):\n",
        "    inp = Input(input_shape)\n",
        "    x = inp\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "        x = Dense(layer_size, activation='relu')(x)\n",
        "        if dropout:\n",
        "            x = Dropout(dropout)(x)\n",
        "    #x = Dense(input_shape)(x)\n",
        "    #x = Add()([x, inp])  # Skip connection\n",
        "    x = Concatenate()([x, inp])  # Skip connection\n",
        "    x = Activation(activation='relu')(x)\n",
        "    out = Dense(layer_sizes[-1], activation=output_activation)(x)\n",
        "    model = Model(inp, out)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_score_Lasso(weights, action_space_withInx, state_dim):\n",
        "    a_space = action_space_withInx[:, :state_dim]\n",
        "    scorItems = np.inner(weights, a_space)\n",
        "\n",
        "    sortIII = np.argsort(scorItems)\n",
        "    Inx_I = sortIII[scorItems.size-(850):]\n",
        "\n",
        "    Inx_Rec_I = action_space_withInx[Inx_I, state_dim ]\n",
        "    return Inx_Rec_I, Inx_I, action_space_withInx\n",
        "\n",
        "def get_score(weights, action_space_withInx, state_dim):\n",
        "    a_space = action_space_withInx[:, :state_dim]\n",
        "    scorItems = np.inner(weights, a_space)  # ret = np.dot(weights, embedding.T)\n",
        "    #Inx_I = np.argmax(scorItems)\n",
        "\n",
        "    sortIII = np.argsort(scorItems)\n",
        "    #print(sortIII)\n",
        "    #sortVVV = np.sort(scorItems)\n",
        "    Inx_I = sortIII[scorItems.size-(K_items):]\n",
        "    #print(\"Inx_I************\")\n",
        "    #print(Inx_I)\n",
        "\n",
        "    Inx_Rec_I = action_space_withInx[Inx_I, state_dim ]\n",
        "    return Inx_Rec_I, Inx_I, action_space_withInx\n",
        "\n",
        "\n",
        "def get_action(s, noise_scale, mu , action_max, num_actions):\n",
        "    a = action_max * mu.predict(s.reshape(1,-1))[0]\n",
        "    return a\n",
        "\n",
        "def ddpg_1(\n",
        "    mu_lr,\n",
        "    q_lr,\n",
        "    decay=0.99,\n",
        "    gamma=gam,\n",
        "    batch_size=bz,\n",
        "    action_noise=0.0,\n",
        "    num_states = num_Features,\n",
        "    num_actions = num_Features):\n",
        "\n",
        "  # Network parameters\n",
        "  X_shape = (num_states)\n",
        "  QA_shape = (num_states + num_actions)\n",
        "\n",
        "  # Main network outputs\n",
        "  #mu_1 = create_vit_Net(X_shape, transformer_layers, projection_dim, num_heads, transformer_units, list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh' ) #mu_1 = ANN2(X_shape,list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh')\n",
        "  mu_1 = ANN2(X_shape,list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh')\n",
        "  q_mu_1 = ANN2(QA_shape, list(hidden_sizes_2)+[1], hidden_activation='relu')\n",
        "\n",
        "  # Target networks\n",
        "  #mu_target_1 = create_vit_Net(X_shape, transformer_layers, projection_dim, num_heads, transformer_units, list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh' ) #mu_1 = ANN2(X_shape,list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh')\n",
        "  mu_target_1 = ANN2(X_shape,list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh')\n",
        "  q_mu_target_1 = ANN2(QA_shape, list(hidden_sizes_2)+[1], hidden_activation='relu')\n",
        "\n",
        "  # Copying weights in,\n",
        "  mu_target_1.set_weights(mu_1.get_weights())\n",
        "  q_mu_target_1.set_weights(q_mu_1.get_weights())\n",
        "\n",
        "  # Train each network separately\n",
        "  mu_optimizer_1 =tf.keras.optimizers.Adam(learning_rate=mu_lr)\n",
        "  q_optimizer_1 = tf.keras.optimizers.Adam(learning_rate=q_lr)\n",
        "\n",
        "  return mu_1, q_mu_1, mu_target_1, q_mu_target_1, mu_optimizer_1, q_optimizer_1\n",
        "\n",
        "def Actor_Critic_optimization(mu, q_mu, mu_target, q_mu_target, mu_optimizer, q_optimizer , X , X2, A, R, D, action_max, q_losses, mu_losses, gamma, decay):\n",
        "  #Actor optimization\n",
        "  with tf.GradientTape() as tape2:\n",
        "    Aprime = action_max * mu(X)\n",
        "    temp = tf.keras.layers.concatenate([X,Aprime],axis=1)  ## x = Concatenate()([u, m])  ## x = Dot(axes=1)([P,Q])\n",
        "    #print( \" temp = \")\n",
        "    #print(temp)\n",
        "    Q = q_mu(temp)\n",
        "    mu_loss =  -tf.reduce_mean(Q)\n",
        "    grads_mu = tape2.gradient(mu_loss,mu.trainable_variables)\n",
        "  mu_losses.append(mu_loss)\n",
        "  mu_optimizer.apply_gradients(zip(grads_mu, mu.trainable_variables))\n",
        "\n",
        "  #Critic Optimization\n",
        "  with tf.GradientTape() as tape:\n",
        "    next_a = action_max * mu_target(X2)\n",
        "    temp = np.concatenate((X2,next_a),axis=1)\n",
        "    #print( \" temp = \")\n",
        "    #print(temp)\n",
        "    #q_target = R + gamma * (1 - D) * q_mu_target(temp)\n",
        "    q_target = R + gamma * q_mu_target(temp)\n",
        "    #print(\" A =\")\n",
        "    #print(A)\n",
        "    #print(\" X = \")\n",
        "    #print(X)\n",
        "    temp2 = np.concatenate((X,A),axis=1)\n",
        "    #print( \" temp2 = \")\n",
        "    #print(temp2)\n",
        "    qvals = q_mu(temp2)\n",
        "    q_loss = tf.reduce_mean((qvals - q_target)**2)\n",
        "    grads_q = tape.gradient(q_loss,q_mu.trainable_variables)\n",
        "  q_optimizer.apply_gradients(zip(grads_q, q_mu.trainable_variables))\n",
        "  q_losses.append(q_loss)\n",
        "\n",
        "  ## Updating both netwokrs\n",
        "  ## updating Critic network\n",
        "\n",
        "  temp1 = np.array(q_mu_target.get_weights())\n",
        "  temp2 = np.array(q_mu.get_weights())\n",
        "  temp3 = decay*temp1 + (1-decay)*temp2\n",
        "  q_mu_target.set_weights(temp3)\n",
        "\n",
        "  # updating Actor network\n",
        "  temp1 = np.array(mu_target.get_weights())\n",
        "  temp2 = np.array(mu.get_weights())\n",
        "  temp3 = decay*temp1 + (1-decay)*temp2\n",
        "  mu_target.set_weights(temp3)\n",
        "\n",
        "  return mu, q_mu, mu_target, q_mu_target, mu_optimizer, q_optimizer, q_losses, mu_losses\n",
        "\n",
        "\n",
        "\n",
        "def switchlr(lr):\n",
        "  if lr < 0.000001:\n",
        "    lr = lr / 10\n",
        "\n",
        "  return lr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/gdrive/MyDrive/Colab Notebooks')\n",
        "\n",
        "# Import your module or file\n",
        "from buffer import *\n",
        "\n",
        "\n",
        "\n",
        "def Main_Loop(\n",
        "    num_train_episodes=n_t_e,\n",
        "    gamma=gam,\n",
        "    switch_rate =s_r,\n",
        "    decay=0.99,\n",
        "    mu_lr=lr,\n",
        "    q_lr=lr,\n",
        "    replay_size=replay_sizeee,\n",
        "    batch_size=bz,\n",
        "    start_steps=start_step,\n",
        "    action_noise=0.0,\n",
        "    max_episode_length=mel):\n",
        "\n",
        "  # get size of state space and action space\n",
        "\n",
        "  state_dim = num_Features    # state_dim = 79\n",
        "  action_space = mTrain[:, :state_dim]   # action_space = mTrain[:, :state_dim]\n",
        "\n",
        "  n_actions = len(action_space)\n",
        "  observation_space = mTrain[:, :state_dim]\n",
        "  n_features = num_Features  # n_features = 79\n",
        "\n",
        "  num_states = state_dim # num_states = env.observation_space.shape[0]\n",
        "  num_actions = n_features # env.action_space.shape[0]\n",
        "  action_max = 1 # action_max = env.action_space.high[0] #################################################*****\n",
        "\n",
        "  mu_2, q_mu_2, mu_target_2, q_mu_target_2, mu_optimizer_2, q_optimizer_2 = ddpg_1(gamma=gam, decay=0.99, mu_lr=lr, q_lr=lr, batch_size=bz, action_noise=0.0, num_states = num_states, num_actions = num_actions)\n",
        "  mu_2_Best, q_mu_2_Best, mu_target_2_Best, q_mu_target_2_Best, mu_optimizer_2, q_optimizer_2 = ddpg_1(gamma=gam, decay=0.99, mu_lr=lr, q_lr=lr, batch_size=bz, action_noise=0.0, num_states = num_states, num_actions = num_actions)\n",
        "\n",
        "\n",
        "  # Experience replay memory\n",
        "  replay_buffer = BasicBuffer_b(size=replay_size,obs_dim=num_states, act_dim=num_actions)\n",
        "\n",
        "\n",
        "  # Main loop: play episode and train\n",
        "  returns = []\n",
        "  q_losses = []\n",
        "  mu_losses = []\n",
        "\n",
        "  returns_simple = []\n",
        "  q_losses_simple = []\n",
        "  mu_losses_simple = []\n",
        "  num_steps = 0\n",
        "\n",
        "  Best_episode_return = -100\n",
        "  Best_episode_Number = 1\n",
        "  i_episode_switch = 1\n",
        "\n",
        "  switch_flag = 0\n",
        "\n",
        "  dis_trueRcomme = []\n",
        "\n",
        "  s , indx = resetFun(mTrain , UserID, state_dim)  # s, episode_return, episode_length, d = resetFun(mTrain , UserID, state_dim), 0, 0, False # env.reset(), 0, 0, False\n",
        "  pos_state_All_epoch = s\n",
        "  count_pos_state_All_epoch = []\n",
        "\n",
        "\n",
        "  max_pos_state_2 = -1\n",
        "  Best_lasso_coef = np.ones(n_features)\n",
        "\n",
        "  lasso_coef = np.ones(num_Features)*1\n",
        "\n",
        "  # --------------------------\n",
        "  s , indx = resetFun(mTrain , UserID, state_dim)\n",
        "  All_Pos_Neg_state_Data = s\n",
        "  All_Pos_Neg_state_Lable = mTrain[indx, num_Features]\n",
        "  #---------------------------\n",
        "\n",
        "  for i_episode in range(num_train_episodes):\n",
        "\n",
        "    mu_lr=changeLR(i_episode)\n",
        "    q_lr = changeLR(i_episode)\n",
        "\n",
        "    # reset env\n",
        "    action_space = mTrain[:, :state_dim]\n",
        "    arrIndex = np.array([[i for i in range(1100)] for j in range(1)])\n",
        "    action_space_Inx = np.append(action_space, arrIndex.T, axis=1)\n",
        "    action_space_withInx = action_space_Inx\n",
        "    action_space_withInx_1 = action_space_withInx\n",
        "    action_space_withInx_2 = action_space_withInx\n",
        "\n",
        "    episode_return_1 = 0\n",
        "    episode_return_2 = 0\n",
        "    episode_length = 0\n",
        "\n",
        "    s , indx = resetFun(mTrain , UserID, state_dim)  # s, episode_return, episode_length, d = resetFun(mTrain , UserID, state_dim), 0, 0, False # env.reset(), 0, 0, False\n",
        "    pos_state_2 = s\n",
        "    s_2 = s\n",
        "\n",
        "    Inx_Lasso_Recomme, Inx_I_2_2, action_space_withInx_2 = get_score_Lasso(lasso_coef, action_space_withInx_2, num_Features)\n",
        "    #print(\"   Lasso Recomme :  \")\n",
        "    #print(Inx_Lasso_Recomme)\n",
        "    Inx_Lasso_Recomme_int = [0] * np.size(Inx_Lasso_Recomme)\n",
        "    for u in range(np.size(Inx_Lasso_Recomme)):\n",
        "      Inx_Lasso_Recomme_int[u] = int(Inx_Lasso_Recomme[u])\n",
        "    selectLassoData = action_space_withInx_2[Inx_Lasso_Recomme_int , :]\n",
        "\n",
        "    #print(\"Inx_Lasso_Recomme  === \")\n",
        "    #print(Inx_Lasso_Recomme)\n",
        "\n",
        "    Pos_Neg_state_Data = s\n",
        "    Pos_Neg_state_Lable = mTrain[indx, num_Features]\n",
        "\n",
        "    while not ((episode_length == max_episode_length)):  #while not (d or (episode_length == max_episode_length)):\n",
        "      # For the first `start_steps` steps, use randomly sampled actions\n",
        "      # in order to encourage exploration.\n",
        "\n",
        "      if num_steps > start_steps:\n",
        "        a_2 = get_action(s_2, action_noise, mu_2 , action_max , num_actions)\n",
        "      else:\n",
        "        a_2 = action_space[random.randrange(1,1100), :]\n",
        "\n",
        "      # Keep track of the number of steps done\n",
        "      num_steps += 1\n",
        "      if num_steps == start_steps:\n",
        "        print(\"USING AGENT ACTIONS NOW\")\n",
        "\n",
        "\n",
        "      # Step the env\n",
        "      Inx_Rec_I_2, Inx_I_2, selectLassoData = get_score(a_2, selectLassoData, state_dim)\n",
        "      # Inx_Rec_I, selectLassoData = get_score_RecommendedItems(a, selectLassoData, state_dim)\n",
        "\n",
        "      (s2_2, r_2 , pos_state_2, selectLassoData, dis_trueRcomme, pos_state_All_epoch, Pos_Neg_state_Data,Pos_Neg_state_Lable,\n",
        "       All_Pos_Neg_state_Data, All_Pos_Neg_state_Lable) = stepFun( Inx_Rec_I_2, Inx_I_2 , s_2 ,pos_state_2, UserID, Data_Reward_Train,\n",
        "                                                                  selectLassoData, dis_trueRcomme, pos_state_All_epoch,Pos_Neg_state_Data,\n",
        "                                                                  Pos_Neg_state_Lable, All_Pos_Neg_state_Data, All_Pos_Neg_state_Lable)\n",
        "\n",
        "\n",
        "      episode_return_2 += r_2\n",
        "      episode_length += 1\n",
        "\n",
        "      d_store = 0  ## in oure problam, there isn't fainal state.\n",
        "\n",
        "      '''# Store experience to replay buffer\n",
        "      replay_buffer_simple.append([s_2, a_2, r_2, s2_2, d_store])'''\n",
        "\n",
        "      # Store experience to replay buffer\n",
        "      replay_buffer.push(s_2, a_2, r_2, s2_2, d_store)\n",
        "\n",
        "\n",
        "      # Assign next state to be the current state on the next round\n",
        "      s_2 = s2_2\n",
        "\n",
        "    print(\"pos_state=\")\n",
        "    print(pos_state_2.shape)\n",
        "\n",
        "    print(\"Pos_Neg_state_Data=\")\n",
        "    print(Pos_Neg_state_Data.shape)\n",
        "\n",
        "    print(\"All_Pos_Neg_state_Data=\")\n",
        "    print(All_Pos_Neg_state_Data.shape)\n",
        "\n",
        "    #--------------------------------------------------------------------\n",
        "    from sklearn.linear_model import Lasso\n",
        "    lasso = Lasso(alpha = 0.001)  # lasso = Lasso(alpha=0.1, normalize=True)\n",
        "    lasso.fit(All_Pos_Neg_state_Data, All_Pos_Neg_state_Lable)\n",
        "    lasso_coef = lasso.coef_\n",
        "\n",
        "    pos_state_2_array = pos_state_2.reshape(-1)\n",
        "    nP = pos_state_2_array.size\n",
        "\n",
        "    if nP > max_pos_state_2:\n",
        "      max_pos_state_2 = nP\n",
        "      Best_lasso_coef = lasso_coef\n",
        "\n",
        "    #--------------------------------------------------------------------\n",
        "    pos_state_All_epoch = np.unique(pos_state_All_epoch, axis=0)\n",
        "    #print(\"pos_state_All_epoch=\")\n",
        "    #print(pos_state_All_epoch.shape)\n",
        "    num_pos , len_pos = pos_state_All_epoch.shape\n",
        "    #print(\"num_pos  =   \", num_pos)\n",
        "    count_pos_state_All_epoch.append(num_pos)\n",
        "\n",
        "    if (episode_length % 5 == 0):\n",
        "        #print(\"Update\")\n",
        "        # Perform the updates\n",
        "\n",
        "        for _ in range(episode_length):\n",
        "\n",
        "          X,A,R,X2,D = replay_buffer.sample(batch_size)\n",
        "          X = np.asarray(X,dtype=np.float32)\n",
        "          A = np.asarray(A,dtype=np.float32)\n",
        "          R = np.asarray(R,dtype=np.float32)\n",
        "          X2 = np.asarray(X2,dtype=np.float32)\n",
        "          D = np.asarray(D,dtype=np.float32)\n",
        "\n",
        "          '''mini_batch_simple = random.sample(replay_buffer_simple, batch_size)\n",
        "          X_simple = np.array([transition[0] for transition in mini_batch_simple])\n",
        "          X2_simple = np.array([transition[3] for transition in mini_batch_simple])\n",
        "          A_simple = np.array([transition[1] for transition in mini_batch_simple])\n",
        "          R_simple = np.array([transition[2] for transition in mini_batch_simple])\n",
        "          D_simple = np.array([transition[4] for transition in mini_batch_simple])'''\n",
        "          mu_2, q_mu_2, mu_target_2, q_mu_target_2, mu_optimizer_2, q_optimizer_2, q_losses_simple, mu_losses_simple = Actor_Critic_optimization(mu_2, q_mu_2, mu_target_2, q_mu_target_2 , mu_optimizer_2, q_optimizer_2, X, X2, A, R, D, action_max, q_losses_simple, mu_losses_simple, gamma,decay)\n",
        "          #\n",
        "    print(\"mu_lr = \", mu_lr)\n",
        "    print(\"Episode:\", i_episode + 1, \"Return_simple  :  \", episode_return_2, 'episode_length:', episode_length)\n",
        "    returns_simple.append(episode_return_2)\n",
        "\n",
        "\n",
        "  return ( lasso_coef, Best_lasso_coef, max_pos_state_2, s_2, returns_simple,q_losses_simple, mu_losses_simple, mu_2, q_mu_2, mu_target_2, q_mu_target_2,episode_return_2,\n",
        "          Best_episode_Number, Best_episode_return, mu_2_Best, q_mu_2_Best, mu_target_2_Best, q_mu_target_2_Best, pos_state_2,\n",
        "          replay_buffer, dis_trueRcomme, count_pos_state_All_epoch, All_Pos_Neg_state_Data, All_Pos_Neg_state_Lable)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def smooth(x):\n",
        "      # last 100\n",
        "  n = len(x)\n",
        "  y = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    start = max(0, i - 99)\n",
        "    y[i] = float(x[start:(i+1)].sum()) / (i - start + 1)\n",
        "  return y\n",
        "\n",
        "(lasso_coef ,Best_lasso_coef, max_pos_state_2, s_2 , returns_simple,q_losses_simple, mu_losses_simple, mu_2, q_mu_2, mu_target_2, q_mu_target_2,episode_return_2,Best_episode_Number,Best_episode_return,\n",
        " mu_2_Best, q_mu_2_Best, mu_target_2_Best, q_mu_target_2_Best, pos_state_2, replay_buffer, dis_trueRcomme, count_pos_state_All_epoch, All_Pos_Neg_state_Data, All_Pos_Neg_state_Lable) = Main_Loop(num_train_episodes=n_t_e) # returns, q_losses,mu_losses = ddpg(lambda : gym.make('Pendulum-v0'),num_train_episodes=50)\n",
        "\n",
        "plt.plot(returns_simple , 'g')\n",
        "plt.plot(smooth(np.array(returns_simple)))\n",
        "plt.title(\"Train returns\")\n",
        "plt.show()\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "avg_reward_list = returns_simple\n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(q_losses_simple)\n",
        "plt.title('q_losses')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(mu_losses_simple)\n",
        "plt.title('mu_losses')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def resetFunTest(pos_state):\n",
        "  size_pos = pos_state.size\n",
        "  if size_pos>num_Features :\n",
        "    mean_pos = np.mean(pos_state, axis = 0)\n",
        "  elif size_pos==num_Features:\n",
        "    mean_pos = pos_state\n",
        "  init_state = mean_pos\n",
        "  return init_state\n",
        "\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha = 0.001)  # lasso = Lasso(alpha=0.1, normalize=True)\n",
        "lasso.fit(All_Pos_Neg_state_Data, All_Pos_Neg_state_Lable)\n",
        "lasso_coef_All = lasso.coef_\n",
        "\n",
        "\n",
        "\n",
        "def get_score_Lasso(weights, action_space_withInx, state_dim):\n",
        "    a_space = action_space_withInx[:, :state_dim]\n",
        "    scorItems = np.inner(weights, a_space)\n",
        "\n",
        "    sortIII = np.argsort(scorItems)\n",
        "    #print(sortIII)\n",
        "    #sortVVV = np.sort(scorItems)\n",
        "    Inx_I = sortIII[scorItems.size-(100):]\n",
        "    #print(\"Inx_I************\")\n",
        "    #print(Inx_I)\n",
        "\n",
        "    Inx_Rec_I = action_space_withInx[Inx_I, state_dim ]\n",
        "    # Delete row at index Inx_I\n",
        "    #print(\"Inx_Rec_I\")\n",
        "    #print(Inx_Rec_I)\n",
        "    #action_space_withInx = np.delete(action_space_withInx, Inx_I, axis=0)\n",
        "    return Inx_Rec_I, Inx_I, action_space_withInx\n",
        "\n",
        "\n",
        "def ddpgTest(\n",
        "    mu = mu_2,\n",
        "    q_mu = q_mu_2,\n",
        "    mu_target = mu_target_2,\n",
        "    q_mu_target = q_mu_target_2,\n",
        "    pos_state = pos_state_2,\n",
        "    replay_buffer = replay_buffer,\n",
        "    num_test_episodes=1,\n",
        "    gamma=gam,\n",
        "    decay=0.99,\n",
        "    mu_lr=lr,\n",
        "    q_lr=lr,\n",
        "    batch_size=bz,\n",
        "    action_noise=0.0,\n",
        "    max_episode_length=1):\n",
        "\n",
        "  # get size of state space and action space\n",
        "\n",
        "  state_dim = num_Features\n",
        "  action_space = mTest[:, :state_dim]\n",
        "  n_actions = len(action_space)\n",
        "  observation_space = mTest[:, :state_dim]\n",
        "  n_features = num_Features\n",
        "\n",
        "  num_states = state_dim # num_states = env.observation_space.shape[0]\n",
        "  num_actions = n_features # env.action_space.shape[0]\n",
        "  action_max = 1 # action_max = env.action_space.high[0] #################################################*****\n",
        "\n",
        "  # Network parameters\n",
        "  X_shape = (num_states)\n",
        "  QA_shape = (num_states + num_actions)\n",
        "\n",
        "  # Main network outputs\n",
        "  mu_test = ANN2(X_shape,list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh')\n",
        "  q_mu_test = ANN2(QA_shape, list(hidden_sizes_2)+[1], hidden_activation='relu')\n",
        "\n",
        "  # Target networks\n",
        "  mu_target_test = ANN2(X_shape,list(hidden_sizes_1)+[num_actions], hidden_activation='relu', output_activation='tanh')\n",
        "  q_mu_target_test = ANN2(QA_shape, list(hidden_sizes_2)+[1], hidden_activation='relu')\n",
        "\n",
        "  # Copying weights in,\n",
        "  mu_test.set_weights(mu.get_weights())\n",
        "  q_mu_test.set_weights(q_mu.get_weights())\n",
        "  mu_target_test.set_weights(mu_target.get_weights())\n",
        "  q_mu_target_test.set_weights(q_mu_target.get_weights())\n",
        "\n",
        "  # Train each network separately\n",
        "  mu_optimizer =tf.keras.optimizers.Adam(learning_rate=mu_lr)\n",
        "  q_optimizer = tf.keras.optimizers.Adam(learning_rate=q_lr)\n",
        "\n",
        "  def get_score_RecommendedItems(weights, action_space_withInx, state_dim):\n",
        "    a_space = action_space_withInx[:, :state_dim]\n",
        "    scorItems = np.inner(weights, a_space)  # ret = np.dot(weights, embedding.T)\n",
        "    Inx_I = np.argsort(scorItems)  # Inx_I = np.argmax(scorItems)\n",
        "    Inx_Rec_I = action_space_withInx[Inx_I, state_dim ]\n",
        "    # Delete row at index Inx_I\n",
        "    action_space_withInx = np.delete(action_space_withInx, Inx_I, axis=0)\n",
        "    return Inx_Rec_I, action_space_withInx\n",
        "\n",
        "  def get_action(s, noise_scale):\n",
        "    a = action_max * mu_test.predict(s.reshape(1,-1))[0]\n",
        "    #a += noise_scale * np.random.randn(num_actions)\n",
        "    #a = np.clip(a, -action_max, action_max)\n",
        "    return a\n",
        "\n",
        "  # Main loop: play episode and train\n",
        "  returns_test = []\n",
        "  q_losses_test = []\n",
        "  mu_losses_test = []\n",
        "  Matrix_Recomm_Index = []\n",
        "  num_steps = 0\n",
        "  for i_episode in range(num_test_episodes):\n",
        "\n",
        "    # reset env\n",
        "    action_space_B = mTest[:, :state_dim]\n",
        "    arrIndex = np.array([[i for i in range(1100)] for j in range(1)])\n",
        "    action_space_Inx_B = np.append(action_space_B, arrIndex.T, axis=1)\n",
        "    action_space_withInx_B = action_space_Inx_B\n",
        "\n",
        "    Inx_Lasso_Recomme, Inx_I_2, action_space_withInx_B = get_score_Lasso(lasso_coef_All, action_space_withInx_B, state_dim)\n",
        "    #print(\"   Lasso Recomme :  \")\n",
        "    #print(Inx_Lasso_Recomme)\n",
        "    Inx_Lasso_Recomme_int = [0] * np.size(Inx_Lasso_Recomme)\n",
        "    for u in range(np.size(Inx_Lasso_Recomme)):\n",
        "      Inx_Lasso_Recomme_int[u] = int(Inx_Lasso_Recomme[u])\n",
        "    selectLassoData = action_space_withInx_B[Inx_Lasso_Recomme_int , :]\n",
        "\n",
        "    s, episode_return_test, episode_length = resetFunTest(pos_state), 0, 0  # s, episode_return, episode_length, d = resetFun(mTrain , UserID, state_dim), 0, 0, False # env.reset(), 0, 0, False\n",
        "\n",
        "    while not ((episode_length == max_episode_length)):  #while not (d or (episode_length == max_episode_length)):\n",
        "\n",
        "      a = get_action(s, action_noise)\n",
        "      print(\"episode_length\")\n",
        "      print(episode_length)\n",
        "      episode_length = episode_length + 1\n",
        "\n",
        "      # Keep track of the number of steps done\n",
        "      num_steps += 1\n",
        "\n",
        "      # Step the env\n",
        "      Inx_Rec_I, selectLassoData = get_score_RecommendedItems(a, selectLassoData, state_dim)\n",
        "\n",
        "      print(Inx_Rec_I)\n",
        "\n",
        "  return (Inx_Rec_I)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def smooth(x):\n",
        "      # last 100\n",
        "  n = len(x)\n",
        "  y = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    start = max(0, i - 99)\n",
        "    y[i] = float(x[start:(i+1)].sum()) / (i - start + 1)\n",
        "  return y\n",
        "\n",
        "Matrix_Recomm_Index = ddpgTest(mu_2, q_mu_2, mu_target_2, q_mu_target_2, pos_state_2, replay_buffer , num_test_episodes=1,gamma=gam,decay=0.99,mu_lr=lr,q_lr=lr,batch_size=bz,action_noise=0.0)\n",
        "\n",
        "print(\"Matrix_Recomm_Index[85:100]\")\n",
        "print(Matrix_Recomm_Index[85:100])\n",
        "print(\"Matrix_Recomm_Index[0:100]\")\n",
        "print(Matrix_Recomm_Index[0:100])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count = 0\n",
        "for i in range (15):\n",
        "  j = 85+i\n",
        "  if (Matrix_Recomm_Index[j]<100):\n",
        "    count = count +1\n",
        "\n",
        "print(\"num true in 15 top_K\")\n",
        "print(count)\n",
        "\n",
        "count = 0\n",
        "for i in range (100):\n",
        "  j = 0+i\n",
        "  if (Matrix_Recomm_Index[j]<100):\n",
        "    count = count +1\n",
        "\n",
        "print(\"num true in 100 top_K\")\n",
        "print(count)\n",
        "\n",
        "\n",
        "\n",
        "#%cd /content/gdrive/MyDrive/NWNew_RB_SPEV_DDPG_lasso_coef_intrain850_intest100\n",
        "#io.savemat(\"NWNew_RB_SPEV_DDPG_lasso_coef_intrain850_intest100_U1.mat\", {\"array\": Matrix_Recomm_Index})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2sDU015Jmeqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}